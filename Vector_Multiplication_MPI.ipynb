{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdQ0/b++4vLjmA2KfBQkUF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrutika-TechSavvy/Google-Colab-Codes/blob/main/Vector_Multiplication_MPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV9bzDvZ35_9",
        "outputId": "1afc2cb3-f33d-40a8-c5fd-a4a2ba73c383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  hwloc-nox libmpich-dev libmpich12 libslurm37\n",
            "Suggested packages:\n",
            "  mpich-doc\n",
            "The following NEW packages will be installed:\n",
            "  hwloc-nox libmpich-dev libmpich12 libslurm37 mpich\n",
            "0 upgraded, 5 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 14.2 MB of archives.\n",
            "After this operation, 102 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libslurm37 amd64 21.08.5-2ubuntu1 [542 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 hwloc-nox amd64 2.7.0-2ubuntu1 [205 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpich12 amd64 4.0-3 [5,866 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mpich amd64 4.0-3 [197 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpich-dev amd64 4.0-3 [7,375 kB]\n",
            "Fetched 14.2 MB in 2s (8,477 kB/s)\n",
            "Selecting previously unselected package libslurm37.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../libslurm37_21.08.5-2ubuntu1_amd64.deb ...\n",
            "Unpacking libslurm37 (21.08.5-2ubuntu1) ...\n",
            "Selecting previously unselected package hwloc-nox.\n",
            "Preparing to unpack .../hwloc-nox_2.7.0-2ubuntu1_amd64.deb ...\n",
            "Unpacking hwloc-nox (2.7.0-2ubuntu1) ...\n",
            "Selecting previously unselected package libmpich12:amd64.\n",
            "Preparing to unpack .../libmpich12_4.0-3_amd64.deb ...\n",
            "Unpacking libmpich12:amd64 (4.0-3) ...\n",
            "Selecting previously unselected package mpich.\n",
            "Preparing to unpack .../archives/mpich_4.0-3_amd64.deb ...\n",
            "Unpacking mpich (4.0-3) ...\n",
            "Selecting previously unselected package libmpich-dev:amd64.\n",
            "Preparing to unpack .../libmpich-dev_4.0-3_amd64.deb ...\n",
            "Unpacking libmpich-dev:amd64 (4.0-3) ...\n",
            "Setting up libslurm37 (21.08.5-2ubuntu1) ...\n",
            "Setting up hwloc-nox (2.7.0-2ubuntu1) ...\n",
            "Setting up libmpich12:amd64 (4.0-3) ...\n",
            "Setting up mpich (4.0-3) ...\n",
            "Setting up libmpich-dev:amd64 (4.0-3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y mpich\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_dynamic.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    int rank, size, n;\n",
        "\n",
        "    // Initialize MPI Environment\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    //  Rank 0 takes input from user\n",
        "    if (rank == 0) {\n",
        "        printf(\"\\nEnter size of matrix (n x n) : \");\n",
        "        scanf(\"%d\", &n);\n",
        "\n",
        "        //  Edge case: size must be >= number of processes\n",
        "        if (n < size) {\n",
        "            printf(\"\\n❌ Number of rows must be >= number of processes!\\n\");\n",
        "        }\n",
        "    }\n",
        "\n",
        "    //  Broadcast size to all processes\n",
        "    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    if (n < size) {\n",
        "        MPI_Finalize();\n",
        "        return 0;\n",
        "    }\n",
        "\n",
        "    // Each process gets n/size rows\n",
        "    int rows_per_proc = n / size;\n",
        "\n",
        "    //  Dynamic allocation\n",
        "    int *matrix = NULL;\n",
        "    int *vector = (int*)malloc(n * sizeof(int));\n",
        "    int *local_matrix = (int*)malloc(rows_per_proc * n * sizeof(int));\n",
        "    int *local_result = (int*)malloc(rows_per_proc * sizeof(int));\n",
        "    int *final_result = NULL;\n",
        "\n",
        "    //  Initialize vector (same in all processes)\n",
        "    for (int i = 0; i < n; i++)\n",
        "        vector[i] = i + 1; // 1,2,3,...\n",
        "\n",
        "    //  Only Rank 0 initializes full matrix\n",
        "    if (rank == 0) {\n",
        "        matrix = (int*)malloc(n * n * sizeof(int));\n",
        "        printf(\"\\nInput Matrix:\\n\");\n",
        "        for (int i = 0; i < n; i++) {\n",
        "            for (int j = 0; j < n; j++) {\n",
        "                matrix[i*n + j] = (i+1) + (j+1); // simple pattern\n",
        "                printf(\"%d \", matrix[i*n + j]);\n",
        "            }\n",
        "            printf(\"\\n\");\n",
        "        }\n",
        "\n",
        "        printf(\"\\nNumber of Processes Running in Parallel: %d\\n\", size);\n",
        "    }\n",
        "\n",
        "    //  Scatter rows among processes\n",
        "    MPI_Scatter(matrix,\n",
        "                rows_per_proc * n,\n",
        "                MPI_INT,\n",
        "                local_matrix,\n",
        "                rows_per_proc * n,\n",
        "                MPI_INT,\n",
        "                0,\n",
        "                MPI_COMM_WORLD);\n",
        "\n",
        "    //  Multiply assigned rows with vector\n",
        "    for (int i = 0; i < rows_per_proc; i++) {\n",
        "        local_result[i] = 0;\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            local_result[i] += local_matrix[i*n + j] * vector[j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    //  Gather results on root\n",
        "    if (rank == 0)\n",
        "        final_result = (int*)malloc(n * sizeof(int));\n",
        "\n",
        "    MPI_Gather(local_result,\n",
        "               rows_per_proc,\n",
        "               MPI_INT,\n",
        "               final_result,\n",
        "               rows_per_proc,\n",
        "               MPI_INT,\n",
        "               0,\n",
        "               MPI_COMM_WORLD);\n",
        "\n",
        "    //  Print final result on root\n",
        "    if (rank == 0) {\n",
        "        printf(\"\\nFinal Output Vector (Matrix × Vector):\\n\");\n",
        "        for (int i = 0; i < n; i++)\n",
        "            printf(\"%d \", final_result[i]);\n",
        "        printf(\"\\n\\n\");\n",
        "    }\n",
        "\n",
        "    //  Free dynamically allocated memory\n",
        "    free(vector);\n",
        "    free(local_matrix);\n",
        "    free(local_result);\n",
        "    if (rank == 0) {\n",
        "        free(matrix);\n",
        "        free(final_result);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMBGPo5e38Lb",
        "outputId": "d7c18b41-cfea-4394-ef88-29d8992e25c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_dynamic.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpicc mpi_dynamic.c -o mpi_dynamic\n"
      ],
      "metadata": {
        "id": "eFfrALTo38m3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun -np 4 ./mpi_dynamic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzSQpO-R38qV",
        "outputId": "1a765c02-f350-4b2f-977a-275923f88a16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------\n",
            "mpirun has detected an attempt to run as root.\n",
            "\n",
            "Running as root is *strongly* discouraged as any mistake (e.g., in\n",
            "defining TMPDIR) or bug can result in catastrophic damage to the OS\n",
            "file system, leaving your system in an unusable state.\n",
            "\n",
            "We strongly suggest that you run mpirun as a non-root user.\n",
            "\n",
            "You can override this protection by adding the --allow-run-as-root option\n",
            "to the cmd line or by setting two environment variables in the following way:\n",
            "the variable OMPI_ALLOW_RUN_AS_ROOT=1 to indicate the desire to override this\n",
            "protection, and OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1 to confirm the choice and\n",
            "add one more layer of certainty that you want to do so.\n",
            "We reiterate our advice against doing so - please proceed at your own risk.\n",
            "--------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}